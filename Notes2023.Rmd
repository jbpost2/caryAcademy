---
title: "Data and Modeling"
output: 
  pdf_document
urlcolor: blue
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```


## What makes something a statistical model?

\vspace{2in}

## What is the difference between prediction and inference?

\vspace{2in}

## Data 

- When modeling, what should our data look like? 

\newpage

# Relating Explanatory Variables to a Response Variable  

Consider the response $Y$ as a random variable.  We'll consider the $x$ values fixed (for any explanatory variable). Our interest is in learning about the relationship between $Y$ and $x$. 

$Y$ is random, so we don't have a **deterministic** relationship... 

```{r, echo = FALSE, out.width = "300px", fig.align='center'}
set.seed(1)
x <- seq(from = -11, to = 17, by = 0.1)
y <- 300 - x^2 -20*x + 1/10*x^3
yerr <- rep(y, 15) + rnorm(length(y)*15, mean = 3, sd = 100)
plot(x, y, type = "l", lwd = 5, col = "blue", ylim = c(-250, 600), main = "Below: Blue line, f(x), is the 'true' relationship between x and y")
lines(rep(x, 15), yerr, type = "p", cex = 0.4)
```

What should we try to relate/model?

\vspace{0.75in}<!--f(x) = E(Y|x)-->


## Approximating $f(x)$

Although the true relationship is most certainly nonlinear, we may be ok approximating the relationship linearly.  For example, consider the same plot as above but between 0 and 5 only:

```{r  echo = FALSE, out.width = "280px", fig.align='center'}
plot(x, y, type = "l", lwd = 5, col = "blue", ylim = c(-250, 600), main = "Blue line, f(x), is the 'true' relationship between x and y", xlim = c(0, 5))
lines(rep(x, 15), yerr, type = "p", cex = 0.4)
```

That's pretty linear.  Consider plot between 5 and 15:

```{r, echo = FALSE, out.width = "280px", fig.align='center'}
plot(x, y, type = "l", lwd = 5, col = "blue", ylim = c(-250, 600), main = "Blue line, f(x), is the 'true' relationship between x and y \n Dashed line is the linear approximation", xlim = c(5, 15))
lines(rep(x, 15), yerr, type = "p", cex = 0.4)
lines(x, y = -10*x+200, lwd = 4, col = "black", lty = 2)
```

Line still does a reasonable job and is often used as a basic approximation.  


# Exploratory Data Analysis (EDA)

What are our first steps with data?

\vspace{1in}

Common steps to EDA

1.  \vspace{0.15in}

2.  \vspace{0.15in}

3.  \vspace{0.15in}

4.  \vspace{0.15in}

5.  \vspace{0.15in}

6.  \vspace{0.15in}

<!--Read data in & understand how your data is stored 
Do basic data validation 
Determine rate of missing values 
Clean data up data as needed 
Investigate distributions 
Apply transformations and repeat previous step

- Day 1 (M, 80 min): Basic EDA concepts/examples. Modeling concepts/ideas. Time for them to practice some EDA on a dataset I give them (titanic data)

-->

\newpage

## Data Intro

This [dataset contains information about used motorcycles and their cost](https://www.kaggle.com/nehalbirla/motorcycle-dataset?select=BIKE+DETAILS.csv). 

From the information page: This data can be used for a lot of purposes such as price prediction to exemplify the use of linear regression in Machine Learning.  The columns in the given dataset are as follows:

- name
- selling price
- year
- seller type
- owner
- km driven
- ex showroom price

The data are available to download from this URL:  
<https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv>


## Read in Data and Explore!  

```{r warning = FALSE, message = FALSE}
library(tidyverse)
bikeData <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv")
select(bikeData, selling_price, year, km_driven, ex_showroom_price, name, everything())
```

Our 'response' variable here is the `selling_price` and we could use the variable `year`, `km_driven`, or `ex_showroom_price` as the explanatory variable.  Let's make some plots and summaries to explore.



\newpage

# Linear Regression

<!--
- Day 2 (W, 80 min): Quick SLR re-intro (although likely done on M a bit). Assumptions/model diagnostics tests/CIs of interest. Transition to Logistic regression. Time for them to fit a model or two in R
-->

\textbf{Recap:} Our goal is to predict a value of $Y$ while including an explanatory variable $x$.  We are assuming we have a sample of $(x_i, y_i)$ pairs, $i = 1,...,n$.  


The Simple Linear Regression (SLR) model can be used:

$$Y_i = \beta_0 + \beta_1x_i + E_i$$
where   

  - $y_i$ is our response for the $i^{th}$ observation
  - $x_i$ is the value of our explanatory variable for the $i^{th}$ observation 
  - $\beta_0$ is the y intercept
  - $\beta_1$ is the slope
  - $E_i\stackrel{iid}\sim N(0,\sigma^2)$

What is important to know from all that??

\vspace{2in}

We **fit** this model to data. That is, find the **best** estimators of $\beta_0$ and $\beta_1$ (and $\sigma^2$) given the data. How to fit the line?

```{r echo = FALSE, out.width = "300px", fig.align='center'}
plot(x = log(bikeData$km_driven), y= log(bikeData$selling_price), main = "SLR model residuals", xlab = "ln(km Driven)", ylab = "ln(Selling Price)")
fit <- lm(log(selling_price) ~ log(km_driven), data = bikeData)
abline(fit)
segments(x0 = log(bikeData$km_driven), x1 = log(bikeData$km_driven), y0 = log(bikeData$selling_price), y1 = predict(fit))
```

\newpage

## Fitting the line 

The (fitted) linear regression model uses $\hat{f}(x) = \hat\beta_0+\hat\beta_1x$. 
Calculus allows us to find the 'least squares' estimators, $\hat\beta_0$ and $\hat\beta_1$ in a nice closed-form!

\vspace{3in}


## Making Inference

What hypothesis are we interested in and why?

\vspace{2in}

How can we form a confidence interval for the quantity of interest?

\vspace{2in}


## Checking assumptions

How can we check our assumptions on the errors?

\vspace{3in}


# Fitting a Linear Regression Model in R

We can fit the model with the `lm()` function. Provide a `formula`

$$response \sim explanatory_variable_equation (\mbox{intercept fit by default})$$

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
bikeData <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv")
bikeData <- bikeData %>% mutate(log_selling_price = log(selling_price), 
                                log_km_driven = log(km_driven))

fit <- lm(log_selling_price ~ log_km_driven, data = bikeData)
```

Determine the fitted model by looking at the `coefficients` element.

```{r}
fit$coefficients
```

Look at the hypothesis test of interest with `summary()`

```{r}
summary(fit)
```

What here is important and why?

\vspace{4in}

Find a confidence interval with `confint()`

```{r}
confint(fit)
```

\newpage

Check conditions! `plot()` on the model fit will work.

```{r, out.width = "500px"}
par(mfrow = c(2,2))
plot(fit)
```


\newpage

# Logistic Regression Model

Used when you have a **binary** response variable

- Using SLR is not appropriate!

Example:

- Consider data about [water potability](https://www.kaggle.com/code/leabenzvi/water-potability-classification)

```{r, message = FALSE}
library(tidyverse)
water <- read_csv("water_potability.csv")
water
```

- Summarize water potability

```{r}
table(water$Potability)
water %>% 
  group_by(Potability) %>% 
  select(Hardness, Chloramines, Potability) %>% 
  summarize(meanH = mean(Hardness), meanC = mean(Chloramines))
```

Why is linear regression not appropriate?

```{r, out.width = "250px",fig.align='center'}
fit <- lm(Potability ~ Hardness, data = water)
ggplot(water, aes(x = Hardness, y = Potability)) + 
  geom_point() +
  geom_smooth(method = "lm")
```

Better view... 

```{r, out.width = "250px",fig.align='center'}
fit <- lm(Potability ~ Hardness, data = water)
ggplot(water, aes(x = Hardness, y = Potability)) + 
  geom_jitter() +
  geom_smooth(method = "lm")
```

\newpage

An even better view of the data is to visualize the proportions of successes as a function of hardness.

```{r, out.width = '320px', fig.align = 'center', warning = FALSE, message = FALSE, echo = FALSE}
get_midpoint <- function(cut_label) {
  mean(as.numeric(unlist(strsplit(gsub("\\(|\\)|\\[|\\]", "", as.character(cut_label)), ","))))
}

get_midpoint_v <- Vectorize(get_midpoint)

water_summarized <- water %>% 
  mutate(Hardness_Cat = cut(Hardness, breaks = 30)) %>%
  group_by(Hardness_Cat) %>%
  summarize(Prop_Potable = mean(Potability), Count = n()) %>%
  mutate(Hardness_Midpoint = get_midpoint_v(Hardness_Cat))

ggplot(water_summarized, aes(x = Hardness_Midpoint, y = Prop_Potable, size = Count)) + 
  geom_point(stat = "identity")
```

- In SLR, we modeled the average of the response as a linear function.  What does the average of the responses represent here? Why does using a linear function not make sense?

\newpage


- Basic Logistic Regression models success probability using the *logistic function*
 
$$P(success|x) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$
 
- This function never goes below 0 and never above 1 - works great for many applications!

- The logistic regression model doesn't have a closed form solution (maximum likelihood often used to fit parameters)  

```{r, echo = FALSE, out.width = '300px', fig.align = 'center'}
x <- seq(0, 1, 0.01)
b0 <- -5
b1 <- 11
exp_fun <- function(x, b0, b1){exp(b0+b1*x)}
plot(x, exp_fun(x, b0, b1)/(1+exp_fun(x, b0, b1)), ylim = c(0,1), xlim = c(0,1), xlab = "x", ylab= "P(Success|x)", main = "b0 = -5, b1 = 11")
```

- Back-solving shows the *logit* or *log-odds* of success is linear in the parameters!

\vspace{3in}

- Coefficient interpretation changes greatly from linear regression model!  
- $\beta_1$ represents a change in the **log-odds of success**  

\newpage

## Hypotheses of Interest

What do you think would indicate that $x$ is related to the probability of success here?

\vspace{3in}

## Fitting a Logistic Regression Model in R

Fit in R using `glm()` with `family = binomial` and a formula just like `lm()`.

```{r}
fit <- glm(Potability ~ Hardness, data = water, family = "binomial")
```

Get coefficients by looking at `coefficients` element:
```{r}
fit$coefficients
```

Get hypothesis test via `summary()`:

```{r}
summary(fit)
```

Get confidence interval for $\beta_1$ with:

```{r}
confint(fit)
```

If we want a probability estimate back, use `predict()` with `type = 'response'`:

```{r}
predict(fit, newdata = data.frame(Hardness = c(200, 300)), type = "response", se.fit = TRUE)
```

Visualize the fit:

```{r, echo = FALSE}
Hardness_Seq <- data.frame(Hardness = seq(min(water$Hardness),
                                          max(water$Hardness),
                                          len=1000))
preds <- predict(fit, newdata = Hardness_Seq, type = "response")
plot_df <- data.frame(preds, Hardness_Seq)
ggplot(water_summarized) + 
  geom_point(stat = "identity", aes(x = Hardness_Midpoint, y = Prop_Potable, size = Count)) + 
  geom_line(data = plot_df, aes(x = Hardness, y = preds))
```

Is a logistic curve!
```{r, echo = FALSE}
Hardness_Seq <- data.frame(Hardness = seq(min(water$Hardness),
                                          5000,
                                          len=100000))
preds <- predict(fit, newdata = Hardness_Seq, type = "response")
plot_df <- data.frame(preds, Hardness_Seq)
ggplot(water_summarized) + 
  geom_point(stat = "identity", aes(x = Hardness_Midpoint, y = Prop_Potable, size = Count)) + 
  geom_line(data = plot_df, aes(x = Hardness, y = preds))
```



